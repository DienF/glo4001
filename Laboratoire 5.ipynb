{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 5 - Kinect\n",
    "\n",
    "Au cours de ce laboratoire, nous allons étudier le capteur Kinect, qui permet \n",
    "d'obtenir des données de profondeur. Connectons-nous d'abord au robot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib nbagg\n",
    "\n",
    "from robmob import robot\n",
    "from robmob.point_cloud import PointCloud\n",
    "from robmob.sensors import KinectDepthSensor, KinectRGBSensor\n",
    "from time import sleep\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.use('nbagg')\n",
    "from matplotlib import cm\n",
    "\n",
    "robot_ip = '192.168.0.114'\n",
    "robot = robot.Robot(robot_ip)\n",
    "robot.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Visualisation\n",
    "\n",
    "Ajoutons deux capteurs au robot, le capteur Kinect RGB qui retourne les images couleur et le capteur Kinect Depth qui retourne la coordonnée en `z` de chaque pixel.\n",
    "\n",
    "> **NOTE** Le système de coordonnées utilisé est $z$ devant, $x$ à droite et $y$ vers le bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth_sensor = KinectDepthSensor()\n",
    "rgb_sensor = KinectRGBSensor()\n",
    "robot.add_sensor(depth_sensor)\n",
    "robot.add_sensor(rgb_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante affiche les données retournées par la Kinect. Dans l'image du haut, la valeur du pixel est directement la distance en mètres. Vous pouvez afficher cette distance en mettant la souris sur le pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_kinect_data(depth_data, rgb_data=None):\n",
    "    if rgb_data is not None:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "        im1 = ax1.imshow(depth_data, aspect='equal')\n",
    "        im2 = ax2.imshow(rgb_data, aspect='equal')\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im1, cax=cbar_ax)\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.imshow(depth_data, aspect='equal')\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appelons cette fonction avec les dernières données du capteur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth_data = depth_sensor.peek_data()\n",
    "rgb_data = rgb_sensor.peek_data()\n",
    "\n",
    "show_kinect_data(depth_data, rgb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE** N'oubliez pas de fermer le graphique interactif avec le bouton bleu.\n",
    "\n",
    "## Partie 2 - Conversion en nuage de points\n",
    "\n",
    "Dans cette partie, nous allons convertir des données de profondeur en `z` en un nuage de points `x y z`.\n",
    "\n",
    "\n",
    "Nous avons déjà toutes les informations nécessaires. Le field-of-view de la Kinect se trouve dans les constantes ```KinectDepthSensor.FOV_X``` et  ```KinectDepthSensor.FOV_Y```. De même, le point principal se se trouve dans les constantes ```KinectDepthSensor.CENTER_X``` et  ```KinectDepthSensor.CENTER_Y```. Ces informations nous permettent déjà de trouver les angles en x et y de chaque pixel par rapport à l'axe optique.\n",
    "\n",
    "\n",
    "La distance `z` est connue, c'est ce qui est retourné directement par la Kinect. À partir des valeurs de `theta` et de `z`, on peut trouver les distances `dx` et `dy` qui représentent la composantes `x` et `y` du vecteur qui part de la caméra vers le point représenté à un pixel donné.\n",
    "\n",
    "> **NOTE** Avec numpy, si on a une matrice **a** de taille 3x3 et un vecteur ligne **b** de taille 1x3, `a * b` représente le fait de multiplier le vecteur `b` element-wise sur chaque ligne. On parle ici de broadcasting en vocabulaire numpy. Cela évite les boucles for, qui sont plus lente que des opérations numpy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_points(depth_image, color_image=None, keep_invalid_points=False):\n",
    "    height, width = depth_image.shape\n",
    "\n",
    "    theta_x = np.arctan((np.arange(width) - KinectDepthSensor.CENTER_X) / KinectDepthSensor.FOV_X)\n",
    "    theta_y = np.arctan((np.arange(height) - KinectDepthSensor.CENTER_Y) / KinectDepthSensor.FOV_Y)\n",
    "    theta_x = np.expand_dims(theta_x, axis=0) # Row matrix\n",
    "    theta_y = np.expand_dims(theta_y, axis=1) # Column matrix\n",
    "\n",
    "    x = depth_image * np.tan(theta_x)\n",
    "    y = depth_image * np.tan(theta_y)\n",
    "    z = depth_image\n",
    "\n",
    "    x_column = np.reshape(x, (-1, 1))\n",
    "    y_column = np.reshape(y, (-1, 1))\n",
    "    z_column = np.reshape(z, (-1, 1))\n",
    "    \n",
    "    points = np.hstack((x_column, y_column, z_column))\n",
    "    \n",
    "   \n",
    "    points_to_keep_mask = np.ones(points.shape[0], dtype=np.bool)\n",
    "    if not keep_invalid_points:\n",
    "         #Removes rows where the z value is 0, meaning unknown distance\n",
    "        points_to_keep_mask = ~(abs(points) < 1e-6)[:,2]\n",
    "        \n",
    "    points = points[points_to_keep_mask]\n",
    "    \n",
    "    if color_image is not None:\n",
    "        colors = np.asarray(color_image).reshape((-1, 3))\n",
    "        colors = colors[points_to_keep_mask]\n",
    "        return points, colors\n",
    "    else:\n",
    "        return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appelez la fonction que vous venez de définir avec une depth image et l'image rgb correspondante. Les array numpy retournées ont un point par ligne, respectivement x, y, z et r, g, b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points, colors = to_points(depth_data, rgb_data)\n",
    "print(points.shape, colors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour visualizer le nuage de points en 3D, il suffit de créer un objet PointCloud et d'appeler la fonction save(). Cela va enregistrer sur le disque le nuage de points en format binaire pour un visualisateur web.\n",
    "\n",
    "> **Attention!** La fonction save() écrase le nuage de points précédemment sauvegardé\n",
    "\n",
    "Une fois la fonction save appelée, il suffit de cliquer sur le lien suivant pour ouvrir le visualisateur.\n",
    "\n",
    "### [POINT CLOUD VIEWER](point_cloud/viewer.html)\n",
    "\n",
    "> **NOTE** Si vous sauvegardez un nouveau nuage de points, il suffit de rafraîchir la page du visualisateur pour l'afficher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PointCloud(points, colors).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Bruit de capteur\n",
    "### Selon z (profondeur)\n",
    "Les données obtenues avec la Kinect, comme avec tous autres capteurs, sont bruitées. De plus, les\n",
    "profondeurs seront quantifiées (ne prendront pas des valeurs continues mais vont varier par des sauts\n",
    "fixes), avec un pas variant avec la profondeur. Afin de mieux voir ces effets, placez la Kinect sur une\n",
    "boite à environ un mètre du mur tel qu’illustré ci-dessous. Assurez-vous, autant que possible, que le\n",
    "montage soit parallèle au mur.\n",
    "\n",
    "![Montage d'obsevation du bruit](./img/kinect-noise.png)\n",
    "\n",
    "Le code suivant affiche la valeur de $z$ au centre de l'image et sur les côtés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth_data = depth_sensor.peek_data()\n",
    "\n",
    "y = math.floor(depth_data.shape[0] / 2)\n",
    "x_center = math.floor(depth_data.shape[1] / 2)\n",
    "x_left = math.floor(depth_data.shape[1] / 5)\n",
    "x_right = depth_data.shape[1] - x_left\n",
    "\n",
    "z_center = depth_data[y, x_center]\n",
    "z_left = depth_data[y, x_left]\n",
    "z_right = depth_data[y, x_right]\n",
    "\n",
    "print(\"Valeurs de z:\")\n",
    "print(\"Gauche: %1.8f m | Centre: %1.8f m | Droite: %1.8f m\" % (z_left, z_center, z_right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répétez la démarche avec la Kinect à 3 mètres du mur, puis à 5 mètres. Est-ce que la précision sur les\n",
    "mesures/grandeurs des pas est la même pour de plus grandes distances? D’après vos observations, est-\n",
    "ce que la taille des sauts est \n",
    "- a) fixe?\n",
    "- b) croit linéairement avec $z$?\n",
    "- c) croit en $z^2$ ?\n",
    "\n",
    "### Selon x\n",
    "Le bruit dans une image produite\n",
    "par une caméra se présente sous\n",
    "plusieurs formes (e.g. distorsion\n",
    "radiale). Étant donné que le calcul\n",
    "de la position en x et en y pour\n",
    "chaque pixel dépend de la valeur\n",
    "bruitée en z et de la position du\n",
    "pixel dans l’image (elle aussi\n",
    "bruitée), il serait intéressant\n",
    "d’observer la précision des valeurs\n",
    "obtenues dans le nuage de point.\n",
    "\n",
    "\n",
    "La fonction suivant prend en argument un nuage de points une coordonnée en pixel $i$ (ligne) $j$ (colonne) et retourne la coordonnée $xyz$ correspondante. Vous pouvez trouver les valeurs de $i$ et $j$ grâce à la visualisation plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def point_at(points, i, j):\n",
    "    return points[i * 640 + j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placez le coté d’une boite au centre\n",
    "optique horizontal de la caméra et\n",
    "utilisez des règles (voir la figure ci-\n",
    "dessous) pour mesurer la position\n",
    "relative réelle d’un repère (e.g. le\n",
    "coin inférieur droit de la boite).\n",
    "\n",
    "![Montage d'obsevation du bruit 2](./img/kinect-noise-2.png)\n",
    "\n",
    "En utilisant les fonctions ```point_at``` et ```to_points```, vérifier si la position mesurée selon x correspond à la valeur dans le nuage de\n",
    "point. Observez quelques mesures à cette position et répétez le processus pour une plus grande distance\n",
    "en x et en z. Est-ce que la précision est similaire dans les deux cas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth_data = depth_sensor.peek_data()\n",
    "show_kinect_data(depth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i, j = 77, 424  #TODO\n",
    "points = to_points(depth_data, keep_invalid_points=True) # keep_invalid_points is used keep 640x480 points \n",
    "                                                         # and be able index the points in i, j\n",
    "\n",
    "print(\"x, y, z: \" + str(point_at(points, i, j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 - Détection de plans avec RANSAC\n",
    "L’extraction de surfaces planes dans des nuages de points est une tâche clef dans le traitement et\n",
    "l’analyse de ces derniers. Dans un environnement intérieur, ces surfaces planes correspondront à des\n",
    "murs, au plancher, et à toutes autres surfaces planes présentes dans le champ visuel de la Kinect.\n",
    "Comme plusieurs équipes utiliseront la Kinect pour naviguer, une des premières tâches consiste à\n",
    "identifier le plancher dans un nuage de point. Cette méthode vous sera donc très utile pour vos projets!\n",
    "\n",
    "\n",
    "Dans la première moitié du cours, nous avons vu l’algorithme RANSAC. L’extraction de surface plane\n",
    "est une application intéressante de RANSAC, car vous cherchez une hypothèse (un plan en 3D) dans un\n",
    "ensemble de donné bruités. La méthode consiste à piger, au hasard, 3 points dans le nuage. Ces trois\n",
    "points définissent, de façon unique, un plan en 3D. S’il y a peu d’inliers à ce plan, cette hypothèse n’est\n",
    "donc pas valide. Si, par contre, beaucoup de points sont des inliers, alors il y a de fortes chances qu’ils\n",
    "constituent un plan.\n",
    "\n",
    "\n",
    "Pour comprendre la fonction suivante, veillez vous référez à [ce site](http://mathworld.wolfram.com/Point-PlaneDistance.html) qui décrit comment trouver la distance entre un plan et un point. Prenez également note qu' il est possible de calculer la distance de tous les points tout en même temps. En effet, dans le lien, on fait le produit scalaire en deux vecteur pour avoir la distance. Cette opération peut également se représenter comme un produit matriciel dans le cas de plusieurs points (la fonction ```numpy.dot```). \n",
    "\n",
    "\n",
    "Le code suivant retourne un masque de booléens représentant les points qui font partie du plan principal. Il est possible par la suite de retrouver ces points avec l'opérateur crochet \n",
    "```\n",
    "points[mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_plane_ransac(points, n_trials=1000, epsilon=1e-2):\n",
    "    best_inliers, best_n_inliers = None, 0\n",
    "    for _ in range(n_trials):\n",
    "        #Select three points - the minimum number of points that define a plane\n",
    "        plane_points = points[np.random.choice(points.shape[0], 3, replace=False)]\n",
    "\n",
    "        #Define the plane by a unit vector\n",
    "        normal_vector = np.cross(plane_points[1] - plane_points[0], plane_points[2] - plane_points[0])\n",
    "        normal_unit_vector = normal_vector / np.linalg.norm(normal_vector)\n",
    "\n",
    "        #Compute distances from plane\n",
    "        distances = np.dot((points - plane_points[0]), normal_unit_vector)\n",
    "\n",
    "        #Find inliers\n",
    "        inliers = abs(distances) < epsilon\n",
    "        n_inliers = np.count_nonzero(inliers)\n",
    "        \n",
    "        if n_inliers > best_n_inliers:\n",
    "            best_inliers, best_n_inliers = inliers, n_inliers\n",
    "\n",
    "    return best_inliers, best_n_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction du plan principal\n",
    "\n",
    "Premièrement, allons chercher des données de la Kinect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth_data = depth_sensor.peek_data()\n",
    "rgb_data = rgb_sensor.peek_data()\n",
    "points, colors = to_points(depth_data, rgb_data)\n",
    "\n",
    "show_kinect_data(depth_data, rgb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le défi est de trouver une bonne valeur de epsilon qui n'est ni trop permissive ni trop restrictive. Modifier la valeur d'epsilon dans le code suivant jusqu'à ce que vous soyez satisfaits du résultats dans le visualisateur de nuages de points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inliers, n_inliers = find_plane_ransac(points, 100, epsilon=2e-1) #TODO change epsilon (in meters)\n",
    "print(\"Found %d inliers\" % n_inliers)\n",
    "\n",
    "\n",
    "points_of_plane = points[inliers]\n",
    "#Choose red color for the points of the principal plane\n",
    "points_of_plane_colors = np.tile(np.array([255, 0 , 0], dtype=np.uint8), (n_inliers, 1))\n",
    "\n",
    "\n",
    "other_points = points[~inliers]        # ~inliers is the opposite boolean mask\n",
    "other_points_colors = colors[~inliers]\n",
    "\n",
    "# put all points in the same array\n",
    "all_points = np.vstack((points_of_plane, other_points))\n",
    "all_colors = np.vstack((points_of_plane_colors, other_points_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PointCloud(all_points, all_colors).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de $n$ plans\n",
    "\n",
    "Le code suivant extrait $n$ plans du nuage de points sans remise et les colore d'une couleur aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_n_planes(points, n, ransac_params={}):\n",
    "    colored_points, colors = None, None\n",
    "    remaining_points = points\n",
    "    \n",
    "    for i in range(n):\n",
    "        inliers, n_inliers = find_plane_ransac(remaining_points, **ransac_params)\n",
    "        points_of_plane = remaining_points[inliers]\n",
    "        remaining_points = remaining_points[~inliers]\n",
    "        \n",
    "        random_color = np.random.randint(100, 200, size=[3], dtype=np.uint8)\n",
    "        new_colors = np.tile(random_color, (n_inliers, 1))\n",
    "        \n",
    "        if colored_points is not None:\n",
    "            colored_points = np.vstack((colored_points, points_of_plane))\n",
    "            colors = np.vstack((colors, new_colors))\n",
    "        else:\n",
    "            colored_points = points_of_plane\n",
    "            colors = new_colors\n",
    "    \n",
    "        if remaining_points.shape[0] < 3:\n",
    "            break\n",
    "        \n",
    "    return colored_points, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiez les valeurs de $epsilon$ et $n\\_trials$ et observez le résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth_data = depth_sensor.peek_data()\n",
    "rgb_data = rgb_sensor.peek_data()\n",
    "points, colors = to_points(depth_data, rgb_data)\n",
    "\n",
    "show_kinect_data(depth_data, rgb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc, cc = color_n_planes(points, 4, ransac_params={'n_trials': 50, 'epsilon': 2e-2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PointCloud(pc, cc).save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
